{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача «Классификация эмоций в текстовых расшифровках голосовых сообщений»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\Taya\\Desktop\\Фразы-эмоции.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1043 entries, 0 to 1042\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Текст фразы  1043 non-null   object\n",
      " 1   Эмоция       1043 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 16.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Текст фразы</th>\n",
       "      <th>Эмоция</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Я не могу поверить, что ты снова опоздал на ва...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Врач, к которому меня направили, оказался груб...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Как же меня раздражает твоя привычка постоянно...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Мне кажется, что некоторые врачи просто зараба...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Мне надоело слушать твои бесконечные жалобы и ...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>В нашей стране медицина находится в ужасном со...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ты думаешь, что можешь просто так взять и уйти...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Врач, к которому меня направили, оказался груб...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Почему ты не можешь понять, что твои слова и д...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Я потратил(а) несколько часов, ожидая приёма у...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Меня бесит, что ты постоянно пытаешься контрол...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Я больше не хочу видеть тебя в своём доме, тво...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Я ненавижу твою ложь и лицемерие, они разрушаю...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Я устал от твоих бесконечных манипуляций и поп...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Я презираю тебя за то, что ты сделал, и не мог...</td>\n",
       "      <td>Недовольство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Я очень разочарован тем, как это прошло. Столь...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Почему никто не предупредил меня заранее? Это ...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Эти постоянные ошибки в системе раздражают! Ко...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Только что сообщили, что придётся переделывать...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Каждый раз одно и то же! Когда уже начнётся по...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Я предупреждал, что так будет, но, похоже, мен...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Невозможно работать в таких условиях! Каждый р...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Почему снова возникла та же проблема? Сколько ...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Кажется, что объясняешь одно и то же, но всё в...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Послушайте, я не понимаю, почему вы так говори...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Опять эти задержки с оплатой! Почему они не мо...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Опоздали с доставкой заказа, хотя обещали прив...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Вчера звонил в поддержку, и они сказали, что п...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Сколько можно переносить встречу? Каждый раз н...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>В банке снова не нашли мой запрос, хотя я прих...</td>\n",
       "      <td>Злость</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Текст фразы        Эмоция\n",
       "0   Я не могу поверить, что ты снова опоздал на ва...  Недовольство\n",
       "1   Врач, к которому меня направили, оказался груб...  Недовольство\n",
       "2   Как же меня раздражает твоя привычка постоянно...  Недовольство\n",
       "3   Мне кажется, что некоторые врачи просто зараба...  Недовольство\n",
       "4   Мне надоело слушать твои бесконечные жалобы и ...  Недовольство\n",
       "5   В нашей стране медицина находится в ужасном со...  Недовольство\n",
       "6   Ты думаешь, что можешь просто так взять и уйти...  Недовольство\n",
       "7   Врач, к которому меня направили, оказался груб...  Недовольство\n",
       "8   Почему ты не можешь понять, что твои слова и д...  Недовольство\n",
       "9   Я потратил(а) несколько часов, ожидая приёма у...  Недовольство\n",
       "10  Меня бесит, что ты постоянно пытаешься контрол...  Недовольство\n",
       "11  Я больше не хочу видеть тебя в своём доме, тво...  Недовольство\n",
       "12  Я ненавижу твою ложь и лицемерие, они разрушаю...  Недовольство\n",
       "13  Я устал от твоих бесконечных манипуляций и поп...  Недовольство\n",
       "14  Я презираю тебя за то, что ты сделал, и не мог...  Недовольство\n",
       "15  Я очень разочарован тем, как это прошло. Столь...        Злость\n",
       "16  Почему никто не предупредил меня заранее? Это ...        Злость\n",
       "17  Эти постоянные ошибки в системе раздражают! Ко...        Злость\n",
       "18  Только что сообщили, что придётся переделывать...        Злость\n",
       "19  Каждый раз одно и то же! Когда уже начнётся по...        Злость\n",
       "20  Я предупреждал, что так будет, но, похоже, мен...        Злость\n",
       "21  Невозможно работать в таких условиях! Каждый р...        Злость\n",
       "22  Почему снова возникла та же проблема? Сколько ...        Злость\n",
       "23  Кажется, что объясняешь одно и то же, но всё в...        Злость\n",
       "24  Послушайте, я не понимаю, почему вы так говори...        Злость\n",
       "25  Опять эти задержки с оплатой! Почему они не мо...        Злость\n",
       "26  Опоздали с доставкой заказа, хотя обещали прив...        Злость\n",
       "27  Вчера звонил в поддержку, и они сказали, что п...        Злость\n",
       "28  Сколько можно переносить встречу? Каждый раз н...        Злость\n",
       "29  В банке снова не нашли мой запрос, хотя я прих...        Злость"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Недовольство', 'Злость', 'Зависть', 'Сочувствие', 'Радость',\n",
       "       'Печаль', 'Интерес', 'Нейтрально'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Эмоция.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Токенизатор для RuBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "# Преобразуем метки в числовой формат\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Эмоция_код\"] = label_encoder.fit_transform(df[\"Эмоция\"])\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"Текст фразы\"].values,\n",
    "    df[\"Эмоция_код\"].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Токенизация\n",
    "def tokenize_texts(texts, tokenizer, max_len=128):\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings[\"input_ids\"], encodings[\"attention_mask\"]\n",
    "\n",
    "train_input_ids, train_attention_masks = tokenize_texts(train_texts, tokenizer)\n",
    "val_input_ids, val_attention_masks = tokenize_texts(val_texts, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx],\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Создание Dataset и DataLoader\n",
    "train_dataset = EmotionDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = EmotionDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Блок: Определение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Taya\\.cache\\huggingface\\hub\\models--DeepPavlov--rubert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        return self.fc(pooled_output)\n",
    "\n",
    "# Инициализация модели\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = EmotionClassifier(num_labels).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 53/53 [08:45<00:00,  9.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 1.647058128185992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Зависть       0.95      0.86      0.90        22\n",
      "      Злость       0.70      0.77      0.73        30\n",
      "     Интерес       0.77      0.43      0.56        23\n",
      "Недовольство       0.00      0.00      0.00         3\n",
      "  Нейтрально       0.58      1.00      0.73        44\n",
      "      Печаль       0.94      0.58      0.71        26\n",
      "     Радость       0.68      0.59      0.63        29\n",
      "  Сочувствие       0.85      0.69      0.76        32\n",
      "\n",
      "    accuracy                           0.72       209\n",
      "   macro avg       0.68      0.61      0.63       209\n",
      "weighted avg       0.75      0.72      0.71       209\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 53/53 [08:09<00:00,  9.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 0.557092342736586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Зависть       1.00      0.77      0.87        22\n",
      "      Злость       0.79      0.77      0.78        30\n",
      "     Интерес       0.89      0.74      0.81        23\n",
      "Недовольство       0.00      0.00      0.00         3\n",
      "  Нейтрально       0.90      0.82      0.86        44\n",
      "      Печаль       0.70      0.62      0.65        26\n",
      "     Радость       0.70      0.97      0.81        29\n",
      "  Сочувствие       0.73      0.94      0.82        32\n",
      "\n",
      "    accuracy                           0.80       209\n",
      "   macro avg       0.71      0.70      0.70       209\n",
      "weighted avg       0.80      0.80      0.79       209\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 53/53 [08:07<00:00,  9.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Loss: 0.2556485989886635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Зависть       0.74      0.91      0.82        22\n",
      "      Злость       0.86      0.80      0.83        30\n",
      "     Интерес       0.72      0.78      0.75        23\n",
      "Недовольство       0.00      0.00      0.00         3\n",
      "  Нейтрально       0.85      0.75      0.80        44\n",
      "      Печаль       0.64      0.88      0.74        26\n",
      "     Радость       0.88      0.72      0.79        29\n",
      "  Сочувствие       0.90      0.84      0.87        32\n",
      "\n",
      "    accuracy                           0.79       209\n",
      "   macro avg       0.70      0.71      0.70       209\n",
      "weighted avg       0.80      0.79      0.79       209\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Training Loss: {total_loss / len(train_loader)}\")\n",
    "        evaluate_model(model, val_loader)\n",
    "\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Запуск обучения\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок: Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"emotion_classifier.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок: Загрузка и предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Taya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\Taya\\AppData\\Local\\Temp\\ipykernel_9924\\2710022559.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(\"emotion_classifier.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Печаль\n"
     ]
    }
   ],
   "source": [
    "# Загрузка модели\n",
    "loaded_model = EmotionClassifier(num_labels)\n",
    "loaded_model.load_state_dict(torch.load(\"emotion_classifier.pth\"))\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Функция предсказания\n",
    "def predict_emotion(text, model, tokenizer):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        prediction = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    return label_encoder.inverse_transform([prediction.cpu().item()])[0]\n",
    "\n",
    "# Пример использования\n",
    "text_example = \"я сейчас буду плакать\"\n",
    "print(predict_emotion(text_example, loaded_model, tokenizer))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
